{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class BaseModel(object):\n",
    "    \"\"\"Generic class for general methods that are not specific to NER\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"Defines self.config and self.logger\n",
    "\n",
    "        Args:\n",
    "            config: (Config instance) class with hyper parameters,\n",
    "                vocab and embeddings\n",
    "\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.logger = config.logger\n",
    "        self.sess   = None\n",
    "        self.saver  = None\n",
    "\n",
    "\n",
    "    def reinitialize_weights(self, scope_name):\n",
    "        \"\"\"Reinitializes the weights of a given layer\"\"\"\n",
    "        variables = tf.contrib.framework.get_variables(scope_name)\n",
    "        init = tf.variables_initializer(variables)\n",
    "        self.sess.run(init)\n",
    "\n",
    "\n",
    "    def add_train_op(self, lr_method, lr, loss, clip=-1):\n",
    "        \"\"\"Defines self.train_op that performs an update on a batch\n",
    "\n",
    "        Args:\n",
    "            lr_method: (string) sgd method, for example \"adam\"\n",
    "            lr: (tf.placeholder) tf.float32, learning rate\n",
    "            loss: (tensor) tf.float32 loss to minimize\n",
    "            clip: (python float) clipping of gradient. If < 0, no clipping\n",
    "\n",
    "        \"\"\"\n",
    "        _lr_m = lr_method.lower() # lower to make sure\n",
    "\n",
    "        with tf.variable_scope(\"train_step\"):\n",
    "            if _lr_m == 'adam': # sgd method\n",
    "                optimizer = tf.train.AdamOptimizer(lr)\n",
    "            elif _lr_m == 'adagrad':\n",
    "                optimizer = tf.train.AdagradOptimizer(lr)\n",
    "            elif _lr_m == 'sgd':\n",
    "                optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "            elif _lr_m == 'rmsprop':\n",
    "                optimizer = tf.train.RMSPropOptimizer(lr)\n",
    "            else:\n",
    "                raise NotImplementedError(\"Unknown method {}\".format(_lr_m))\n",
    "\n",
    "            if clip > 0: # gradient clipping if clip is positive\n",
    "                grads, vs     = zip(*optimizer.compute_gradients(loss))\n",
    "                grads, gnorm  = tf.clip_by_global_norm(grads, clip)\n",
    "                self.train_op = optimizer.apply_gradients(zip(grads, vs))\n",
    "            else:\n",
    "                self.train_op = optimizer.minimize(loss)\n",
    "\n",
    "\n",
    "    def initialize_session(self):\n",
    "        \"\"\"Defines self.sess and initialize the variables\"\"\"\n",
    "        self.logger.info(\"Initializing tf session\")\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "    def restore_session(self, dir_model):\n",
    "        \"\"\"Reload weights into session\n",
    "\n",
    "        Args:\n",
    "            sess: tf.Session()\n",
    "            dir_model: dir with weights\n",
    "\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Reloading the latest trained model...\")\n",
    "        self.saver.restore(self.sess, dir_model)\n",
    "\n",
    "\n",
    "    def save_session(self):\n",
    "        \"\"\"Saves session = weights\"\"\"\n",
    "        if not os.path.exists(self.config.dir_model):\n",
    "            os.makedirs(self.config.dir_model)\n",
    "        self.saver.save(self.sess, self.config.dir_model)\n",
    "\n",
    "\n",
    "    def close_session(self):\n",
    "        \"\"\"Closes the session\"\"\"\n",
    "        self.sess.close()\n",
    "\n",
    "\n",
    "    def add_summary(self):\n",
    "        \"\"\"Defines variables for Tensorboard\n",
    "\n",
    "        Args:\n",
    "            dir_output: (string) where the results are written\n",
    "\n",
    "        \"\"\"\n",
    "        self.merged      = tf.summary.merge_all()\n",
    "        self.file_writer = tf.summary.FileWriter(self.config.dir_output,\n",
    "                self.sess.graph)\n",
    "\n",
    "\n",
    "    def train(self, train, dev):\n",
    "        \"\"\"Performs training with early stopping and lr exponential decay\n",
    "\n",
    "        Args:\n",
    "            train: dataset that yields tuple of (sentences, tags)\n",
    "            dev: dataset\n",
    "\n",
    "        \"\"\"\n",
    "        best_score = 0\n",
    "        nepoch_no_imprv = 0 # for early stopping\n",
    "        self.add_summary() # tensorboard\n",
    "\n",
    "        for epoch in range(self.config.nepochs):\n",
    "            self.logger.info(\"Epoch {:} out of {:}\".format(epoch + 1,\n",
    "                        self.config.nepochs))\n",
    "\n",
    "            score = self.run_epoch(train, dev, epoch)\n",
    "            self.config.lr *= self.config.lr_decay # decay learning rate\n",
    "\n",
    "            # early stopping and saving best parameters\n",
    "            if score >= best_score:\n",
    "                nepoch_no_imprv = 0\n",
    "                self.save_session()\n",
    "                best_score = score\n",
    "                self.logger.info(\"- new best score!\")\n",
    "            else:\n",
    "                nepoch_no_imprv += 1\n",
    "                if nepoch_no_imprv >= self.config.nepoch_no_imprv:\n",
    "                    self.logger.info(\"- early stopping {} epochs without \"\\\n",
    "                            \"improvement\".format(nepoch_no_imprv))\n",
    "                    break\n",
    "\n",
    "\n",
    "    def evaluate(self, test):\n",
    "        \"\"\"Evaluate model on test set\n",
    "\n",
    "        Args:\n",
    "            test: instance of class Dataset\n",
    "\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Testing model over test set\")\n",
    "        metrics = self.run_evaluate(test)\n",
    "        msg = \" - \".join([\"{} {:04.2f}\".format(k, v)\n",
    "                for k, v in metrics.items()])\n",
    "        self.logger.info(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load model/data_utils\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# shared global variables to be imported from model also\n",
    "UNK = \"$UNK$\"\n",
    "NUM = \"$NUM$\"\n",
    "NONE = \"O\"\n",
    "\n",
    "\n",
    "# special error message\n",
    "class MyIOError(Exception):\n",
    "    def __init__(self, filename):\n",
    "        # custom error message\n",
    "        message = \"\"\"\n",
    "ERROR: Unable to locate file {}.\n",
    "\n",
    "FIX: Have you tried running python build_data.py first?\n",
    "This will build vocab file from your train, test and dev sets and\n",
    "trimm your word vectors.\n",
    "\"\"\".format(filename)\n",
    "        super(MyIOError, self).__init__(message)\n",
    "\n",
    "\n",
    "class CoNLLDataset(object):\n",
    "    \"\"\"Class that iterates over CoNLL Dataset\n",
    "\n",
    "    __iter__ method yields a tuple (words, tags)\n",
    "        words: list of raw words\n",
    "        tags: list of raw tags\n",
    "\n",
    "    If processing_word and processing_tag are not None,\n",
    "    optional preprocessing is appplied\n",
    "\n",
    "    Example:\n",
    "        ```python\n",
    "        data = CoNLLDataset(filename)\n",
    "        for sentence, tags in data:\n",
    "            pass\n",
    "        ```\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, filename, processing_word=None, processing_tag=None,\n",
    "                 max_iter=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            filename: path to the file\n",
    "            processing_words: (optional) function that takes a word as input\n",
    "            processing_tags: (optional) function that takes a tag as input\n",
    "            max_iter: (optional) max number of sentences to yield\n",
    "\n",
    "        \"\"\"\n",
    "        self.filename = filename\n",
    "        self.processing_word = processing_word\n",
    "        self.processing_tag = processing_tag\n",
    "        self.max_iter = max_iter\n",
    "        self.length = None\n",
    "\n",
    "    def __iter__(self):\n",
    "        niter = 0\n",
    "        with open(self.filename, encoding='utf-8') as f:\n",
    "            words, tags = [], []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if (len(line) == 0 or line.startswith(\"-DOCSTART-\")):\n",
    "                    if len(words) != 0:\n",
    "                        niter += 1\n",
    "                        if self.max_iter is not None and niter > self.max_iter:\n",
    "                            break\n",
    "                        yield words, tags\n",
    "                        words, tags = [], []\n",
    "                else:\n",
    "                    ls = line.split(' ')\n",
    "                    word, tag = ls[0],ls[-1]\n",
    "                    if self.processing_word is not None:\n",
    "                        word = self.processing_word(word)\n",
    "                    if self.processing_tag is not None:\n",
    "                        tag = self.processing_tag(tag)\n",
    "                    words += [word]\n",
    "                    tags += [tag]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Iterates once over the corpus to set and store length\"\"\"\n",
    "        if self.length is None:\n",
    "            self.length = 0\n",
    "            for _ in self:\n",
    "                self.length += 1\n",
    "\n",
    "        return self.length\n",
    "\n",
    "\n",
    "def get_vocabs(datasets):\n",
    "    \"\"\"Build vocabulary from an iterable of datasets objects\n",
    "\n",
    "    Args:\n",
    "        datasets: a list of dataset objects\n",
    "\n",
    "    Returns:\n",
    "        a set of all the words in the dataset\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Building vocab...\")\n",
    "    vocab_words = set()\n",
    "    vocab_tags = set()\n",
    "    for dataset in datasets:\n",
    "        for words, tags in dataset:\n",
    "            vocab_words.update(words)\n",
    "            vocab_tags.update(tags)\n",
    "    print(\"- done. {} tokens\".format(len(vocab_words)))\n",
    "    return vocab_words, vocab_tags\n",
    "\n",
    "\n",
    "def get_char_vocab(dataset):\n",
    "    \"\"\"Build char vocabulary from an iterable of datasets objects\n",
    "\n",
    "    Args:\n",
    "        dataset: a iterator yielding tuples (sentence, tags)\n",
    "\n",
    "    Returns:\n",
    "        a set of all the characters in the dataset\n",
    "\n",
    "    \"\"\"\n",
    "    vocab_char = set()\n",
    "    for words, _ in dataset:\n",
    "        for word in words:\n",
    "            vocab_char.update(word)\n",
    "\n",
    "    return vocab_char\n",
    "\n",
    "\n",
    "def get_glove_vocab(filename):\n",
    "    \"\"\"Load vocab from file\n",
    "\n",
    "    Args:\n",
    "        filename: path to the glove vectors\n",
    "\n",
    "    Returns:\n",
    "        vocab: set() of strings\n",
    "    \"\"\"\n",
    "    print(\"Building vocab...\")\n",
    "    vocab = set()\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word = line.strip().split(' ')[0]\n",
    "            vocab.add(word)\n",
    "    print(\"- done. {} tokens\".format(len(vocab)))\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def write_vocab(vocab, filename):\n",
    "    \"\"\"Writes a vocab to a file\n",
    "\n",
    "    Writes one word per line.\n",
    "\n",
    "    Args:\n",
    "        vocab: iterable that yields word\n",
    "        filename: path to vocab file\n",
    "\n",
    "    Returns:\n",
    "        write a word per line\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Writing vocab...\")\n",
    "    with open(filename, \"w\", encoding='utf-8') as f:\n",
    "        for i, word in enumerate(vocab):\n",
    "            if i != len(vocab) - 1:\n",
    "                f.write(\"{}\\n\".format(word))\n",
    "            else:\n",
    "                f.write(word)\n",
    "    print(\"- done. {} tokens\".format(len(vocab)))\n",
    "\n",
    "\n",
    "def load_vocab(filename):\n",
    "    \"\"\"Loads vocab from a file\n",
    "\n",
    "    Args:\n",
    "        filename: (string) the format of the file must be one word per line.\n",
    "\n",
    "    Returns:\n",
    "        d: dict[word] = index\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        d = dict()\n",
    "        with open(filename, encoding='utf-8') as f:\n",
    "            for idx, word in enumerate(f):\n",
    "                word = word.strip()\n",
    "                d[word] = idx\n",
    "\n",
    "    except IOError:\n",
    "        raise MyIOError(filename)\n",
    "    return d\n",
    "\n",
    "\n",
    "def export_trimmed_glove_vectors(vocab, glove_filename, trimmed_filename, dim):\n",
    "    \"\"\"Saves glove vectors in numpy array\n",
    "\n",
    "    Args:\n",
    "        vocab: dictionary vocab[word] = index\n",
    "        glove_filename: a path to a glove file\n",
    "        trimmed_filename: a path where to store a matrix in npy\n",
    "        dim: (int) dimension of embeddings\n",
    "\n",
    "    \"\"\"\n",
    "    embeddings = np.zeros([len(vocab), dim])\n",
    "    with open(glove_filename, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(' ')\n",
    "            word = line[0]\n",
    "            embedding = [float(x) for x in line[1:]]\n",
    "            if word in vocab:\n",
    "                word_idx = vocab[word]\n",
    "                embeddings[word_idx] = np.asarray(embedding)\n",
    "\n",
    "    np.savez_compressed(trimmed_filename, embeddings=embeddings)\n",
    "\n",
    "\n",
    "def get_trimmed_glove_vectors(filename):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        filename: path to the npz file\n",
    "\n",
    "    Returns:\n",
    "        matrix of embeddings (np array)\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with np.load(filename) as data:\n",
    "            return data[\"embeddings\"]\n",
    "\n",
    "    except IOError:\n",
    "        raise MyIOError(filename)\n",
    "\n",
    "\n",
    "def get_processing_word(vocab_words=None, vocab_chars=None,\n",
    "                    lowercase=False, chars=False, allow_unk=True):\n",
    "    \"\"\"Return lambda function that transform a word (string) into list,\n",
    "    or tuple of (list, id) of int corresponding to the ids of the word and\n",
    "    its corresponding characters.\n",
    "\n",
    "    Args:\n",
    "        vocab: dict[word] = idx\n",
    "\n",
    "    Returns:\n",
    "        f(\"cat\") = ([12, 4, 32], 12345)\n",
    "                 = (list of char ids, word id)\n",
    "\n",
    "    \"\"\"\n",
    "    def f(word):\n",
    "        # 1. preprocess word\n",
    "        if lowercase:\n",
    "            word = word.lower()\n",
    "        if word.isdigit():\n",
    "            word = NUM\n",
    "\n",
    "        # 2. get id of word\n",
    "        if vocab_words is not None:\n",
    "            if word in vocab_words:\n",
    "                word = vocab_words[word]\n",
    "            else:\n",
    "                if allow_unk:\n",
    "                    word = vocab_words[UNK]\n",
    "                else:\n",
    "                    raise Exception(\"Unknow key is not allowed. Check that \"\\\n",
    "                                    \"your vocab (tags?) is correct\")\n",
    "      \n",
    "        return word\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def _pad_sequences(sequences, pad_tok, max_length):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sequences: a generator of list or tuple\n",
    "        pad_tok: the char to pad with\n",
    "\n",
    "    Returns:\n",
    "        a list of list where each sublist has same length\n",
    "    \"\"\"\n",
    "    sequence_padded, sequence_length = [], []\n",
    "\n",
    "    for seq in sequences:\n",
    "        seq = list(seq)\n",
    "        seq_ = seq[:max_length] + [pad_tok]*max(max_length - len(seq), 0)\n",
    "        sequence_padded +=  [seq_]\n",
    "        sequence_length += [min(len(seq), max_length)]\n",
    "\n",
    "    return sequence_padded, sequence_length\n",
    "\n",
    "\n",
    "def pad_sequences(sequences, pad_tok, nlevels=1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sequences: a generator of list or tuple\n",
    "        pad_tok: the char to pad with\n",
    "        nlevels: \"depth\" of padding, for the case where we have characters ids\n",
    "\n",
    "    Returns:\n",
    "        a list of list where each sublist has same length\n",
    "\n",
    "    \"\"\"\n",
    "    if nlevels == 1:\n",
    "        max_length = max(map(lambda x : len(x), sequences))\n",
    "        sequence_padded, sequence_length = _pad_sequences(sequences,\n",
    "                                            pad_tok, max_length)\n",
    "\n",
    "    elif nlevels == 2:\n",
    "        max_length_word = max([max(map(lambda x: len(x), seq))\n",
    "                               for seq in sequences])\n",
    "        sequence_padded, sequence_length = [], []\n",
    "        for seq in sequences:\n",
    "            # all words are same length now\n",
    "            sp, sl = _pad_sequences(seq, pad_tok, max_length_word)\n",
    "            sequence_padded += [sp]\n",
    "            sequence_length += [sl]\n",
    "\n",
    "        max_length_sentence = max(map(lambda x : len(x), sequences))\n",
    "        sequence_padded, _ = _pad_sequences(sequence_padded,\n",
    "                [pad_tok]*max_length_word, max_length_sentence)\n",
    "        sequence_length, _ = _pad_sequences(sequence_length, 0,\n",
    "                max_length_sentence)\n",
    "\n",
    "    return sequence_padded, sequence_length\n",
    "\n",
    "\n",
    "def minibatches(data, minibatch_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data: generator of (sentence, tags) tuples\n",
    "        minibatch_size: (int)\n",
    "\n",
    "    Yields:\n",
    "        list of tuples\n",
    "\n",
    "    \"\"\"\n",
    "    x_batch, y_batch = [], []\n",
    "    for (x, y) in data:\n",
    "        if len(x_batch) == minibatch_size:\n",
    "            yield x_batch, y_batch\n",
    "            x_batch, y_batch = [], []\n",
    "\n",
    "        if type(x[0]) == tuple:\n",
    "            x = zip(*x)\n",
    "        x_batch += [x]\n",
    "        y_batch += [y]\n",
    "\n",
    "    if len(x_batch) != 0:\n",
    "        yield x_batch, y_batch\n",
    "\n",
    "\n",
    "def get_chunk_type(tok, idx_to_tag):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        tok: id of token, ex 4\n",
    "        idx_to_tag: dictionary {4: \"B-PER\", ...}\n",
    "\n",
    "    Returns:\n",
    "        tuple: \"B\", \"PER\"\n",
    "\n",
    "    \"\"\"\n",
    "    tag_name = idx_to_tag[tok]\n",
    "    tag_class = tag_name.split('-')[0]\n",
    "    tag_type = tag_name.split('-')[-1]\n",
    "    return tag_class, tag_type\n",
    "\n",
    "\n",
    "def get_chunks(seq, tags):\n",
    "    \"\"\"Given a sequence of tags, group entities and their position\n",
    "\n",
    "    Args:\n",
    "        seq: [4, 4, 0, 0, ...] sequence of labels\n",
    "        tags: dict[\"O\"] = 4\n",
    "\n",
    "    Returns:\n",
    "        list of (chunk_type, chunk_start, chunk_end)\n",
    "\n",
    "    Example:\n",
    "        seq = [4, 5, 0, 3]\n",
    "        tags = {\"B-PER\": 4, \"I-PER\": 5, \"B-LOC\": 3}\n",
    "        result = [(\"PER\", 0, 2), (\"LOC\", 3, 4)]\n",
    "\n",
    "    \"\"\"\n",
    "    default = tags[NONE]\n",
    "    idx_to_tag = {idx: tag for tag, idx in tags.items()}\n",
    "    chunks = []\n",
    "    chunk_type, chunk_start = None, None\n",
    "    for i, tok in enumerate(seq):\n",
    "        # End of a chunk 1\n",
    "        if tok == default and chunk_type is not None:\n",
    "            # Add a chunk.\n",
    "            chunk = (chunk_type, chunk_start, i)\n",
    "            chunks.append(chunk)\n",
    "            chunk_type, chunk_start = None, None\n",
    "\n",
    "        # End of a chunk + start of a chunk!\n",
    "        elif tok != default:\n",
    "            tok_chunk_class, tok_chunk_type = get_chunk_type(tok, idx_to_tag)\n",
    "            if chunk_type is None:\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "            elif tok_chunk_type != chunk_type or tok_chunk_class == \"B\":\n",
    "                chunk = (chunk_type, chunk_start, i)\n",
    "                chunks.append(chunk)\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # end condition\n",
    "    if chunk_type is not None:\n",
    "        chunk = (chunk_type, chunk_start, len(seq))\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load model/general_utils.py\n",
    "import time\n",
    "import sys\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_logger(filename):\n",
    "    \"\"\"Return a logger instance that writes in filename\n",
    "\n",
    "    Args:\n",
    "        filename: (string) path to log.txt\n",
    "\n",
    "    Returns:\n",
    "        logger: (instance of logger)\n",
    "\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger('logger')\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logging.basicConfig(format='%(message)s', level=logging.DEBUG)\n",
    "    handler = logging.FileHandler(filename)\n",
    "    handler.setLevel(logging.DEBUG)\n",
    "    handler.setFormatter(logging.Formatter(\n",
    "            '%(asctime)s:%(levelname)s: %(message)s'))\n",
    "    logging.getLogger().addHandler(handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "\n",
    "class Progbar(object):\n",
    "    \"\"\"Progbar class copied from keras (https://github.com/fchollet/keras/)\n",
    "\n",
    "    Displays a progress bar.\n",
    "    Small edit : added strict arg to update\n",
    "    # Arguments\n",
    "        target: Total number of steps expected.\n",
    "        interval: Minimum visual progress update interval (in seconds).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target, width=30, verbose=1):\n",
    "        self.width = width\n",
    "        self.target = target\n",
    "        self.sum_values = {}\n",
    "        self.unique_values = []\n",
    "        self.start = time.time()\n",
    "        self.total_width = 0\n",
    "        self.seen_so_far = 0\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def update(self, current, values=[], exact=[], strict=[]):\n",
    "        \"\"\"\n",
    "        Updates the progress bar.\n",
    "        # Arguments\n",
    "            current: Index of current step.\n",
    "            values: List of tuples (name, value_for_last_step).\n",
    "                The progress bar will display averages for these values.\n",
    "            exact: List of tuples (name, value_for_last_step).\n",
    "                The progress bar will display these values directly.\n",
    "        \"\"\"\n",
    "\n",
    "        for k, v in values:\n",
    "            if k not in self.sum_values:\n",
    "                self.sum_values[k] = [v * (current - self.seen_so_far),\n",
    "                                      current - self.seen_so_far]\n",
    "                self.unique_values.append(k)\n",
    "            else:\n",
    "                self.sum_values[k][0] += v * (current - self.seen_so_far)\n",
    "                self.sum_values[k][1] += (current - self.seen_so_far)\n",
    "        for k, v in exact:\n",
    "            if k not in self.sum_values:\n",
    "                self.unique_values.append(k)\n",
    "            self.sum_values[k] = [v, 1]\n",
    "\n",
    "        for k, v in strict:\n",
    "            if k not in self.sum_values:\n",
    "                self.unique_values.append(k)\n",
    "            self.sum_values[k] = v\n",
    "\n",
    "        self.seen_so_far = current\n",
    "\n",
    "        now = time.time()\n",
    "        if self.verbose == 1:\n",
    "            prev_total_width = self.total_width\n",
    "            sys.stdout.write(\"\\b\" * prev_total_width)\n",
    "            sys.stdout.write(\"\\r\")\n",
    "\n",
    "            numdigits = int(np.floor(np.log10(self.target))) + 1\n",
    "            barstr = '%%%dd/%%%dd [' % (numdigits, numdigits)\n",
    "            bar = barstr % (current, self.target)\n",
    "            prog = float(current)/self.target\n",
    "            prog_width = int(self.width*prog)\n",
    "            if prog_width > 0:\n",
    "                bar += ('='*(prog_width-1))\n",
    "                if current < self.target:\n",
    "                    bar += '>'\n",
    "                else:\n",
    "                    bar += '='\n",
    "            bar += ('.'*(self.width-prog_width))\n",
    "            bar += ']'\n",
    "            sys.stdout.write(bar)\n",
    "            self.total_width = len(bar)\n",
    "\n",
    "            if current:\n",
    "                time_per_unit = (now - self.start) / current\n",
    "            else:\n",
    "                time_per_unit = 0\n",
    "            eta = time_per_unit*(self.target - current)\n",
    "            info = ''\n",
    "            if current < self.target:\n",
    "                info += ' - ETA: %ds' % eta\n",
    "            else:\n",
    "                info += ' - %ds' % (now - self.start)\n",
    "            for k in self.unique_values:\n",
    "                if type(self.sum_values[k]) is list:\n",
    "                    info += ' - %s: %.4f' % (k,\n",
    "                        self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n",
    "                else:\n",
    "                    info += ' - %s: %s' % (k, self.sum_values[k])\n",
    "\n",
    "            self.total_width += len(info)\n",
    "            if prev_total_width > self.total_width:\n",
    "                info += ((prev_total_width-self.total_width) * \" \")\n",
    "\n",
    "            sys.stdout.write(info)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            if current >= self.target:\n",
    "                sys.stdout.write(\"\\n\")\n",
    "\n",
    "        if self.verbose == 2:\n",
    "            if current >= self.target:\n",
    "                info = '%ds' % (now - self.start)\n",
    "                for k in self.unique_values:\n",
    "                    info += ' - %s: %.4f' % (k,\n",
    "                        self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n",
    "                sys.stdout.write(info + \"\\n\")\n",
    "\n",
    "    def add(self, n, values=[]):\n",
    "        self.update(self.seen_so_far+n, values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load model/config.py\n",
    "import os\n",
    "                                                                                                                            \n",
    "class Config():\n",
    "    def __init__(self, load=True):\n",
    "        \"\"\"Initialize hyperparameters and load vocabs\n",
    "\n",
    "        Args:\n",
    "            load_embeddings: (bool) if True, load embeddings into\n",
    "                np array, else None\n",
    "\n",
    "        \"\"\"\n",
    "        # directory for training outputs\n",
    "        if not os.path.exists(self.dir_output):\n",
    "            os.makedirs(self.dir_output)\n",
    "\n",
    "        # create instance of logger\n",
    "        self.logger = get_logger(self.path_log)\n",
    "\n",
    "        # load if requested (default)\n",
    "        if load:\n",
    "            self.load()\n",
    "\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"Loads vocabulary, processing functions and embeddings\n",
    "\n",
    "        Supposes that build_data.py has been run successfully and that\n",
    "        the corresponding files have been created (vocab and trimmed GloVe\n",
    "        vectors)\n",
    "\n",
    "        \"\"\"\n",
    "        # 1. vocabulary\n",
    "        self.vocab_words = load_vocab(self.filename_words)\n",
    "        self.vocab_tags  = load_vocab(self.filename_tags)\n",
    "        self.vocab_chars = load_vocab(self.filename_chars)\n",
    "\n",
    "        self.nwords     = len(self.vocab_words)\n",
    "        self.nchars     = len(self.vocab_chars)\n",
    "        self.ntags      = len(self.vocab_tags)\n",
    "\n",
    "        # 2. get processing functions that map str -> id\n",
    "        self.processing_word = get_processing_word(self.vocab_words,\n",
    "                self.vocab_chars, lowercase=True, chars=self.use_chars)\n",
    "        self.processing_tag  = get_processing_word(self.vocab_tags,\n",
    "                lowercase=False, allow_unk=False)\n",
    "\n",
    "        # 3. get pre-trained embeddings\n",
    "        self.embeddings = (get_trimmed_glove_vectors(self.filename_trimmed)\n",
    "                if self.use_pretrained else None)\n",
    "\n",
    "\n",
    "    # general config\n",
    "    dir_output = \"results/test/\"\n",
    "    dir_model  = dir_output + \"model.weights/\"\n",
    "    path_log   = dir_output + \"log.txt\"\n",
    "\n",
    "    # embeddings\n",
    "    dim_word = 300\n",
    "    dim_char = 100\n",
    "\n",
    "    # glove files\n",
    "    filename_glove = \"data/glove.6B/glove.6B.{}d.txt\".format(dim_word)\n",
    "    # trimmed embeddings (created from glove_filename with build_data.py)\n",
    "    filename_trimmed = \"data/glove.6B.{}d.trimmed.npz\".format(dim_word)\n",
    "    use_pretrained = True\n",
    "\n",
    "    # dataset\n",
    "    \n",
    "    filename_train = 'data/CONLL/train.txt'\n",
    "    filename_dev = 'data/CONLL/valid.txt'\n",
    "    filename_test = 'data/CONLL/test.txt'\n",
    "\n",
    "\n",
    "    max_iter = None # if not None, max number of examples in Dataset\n",
    "\n",
    "    # vocab (created from dataset with build_data.py)\n",
    "    filename_words = \"data/words.txt\"\n",
    "    filename_tags = \"data/tags.txt\"\n",
    "    filename_chars = \"data/chars.txt\"\n",
    "\n",
    "    # training\n",
    "    train_embeddings = False\n",
    "    nepochs          = 15\n",
    "    dropout          = 0.5\n",
    "    batch_size       = 20\n",
    "    lr_method        = \"adam\"\n",
    "    lr               = 0.001\n",
    "    lr_decay         = 0.9\n",
    "    clip             = -1 # if negative, no clipping\n",
    "    nepoch_no_imprv  = 3\n",
    "\n",
    "    # model hyperparameters\n",
    "    hidden_size_char = 100 # lstm on chars\n",
    "    hidden_size_lstm = 300 # lstm on word embeddings\n",
    "\n",
    "    # NOTE: if both chars and crf, only 1.6x slower on GPU\n",
    "    use_crf = False # if crf, training is 1.7x slower on CPU\n",
    "    use_chars = False # if char embedding, training is 3.5x slower on CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class NERModel(BaseModel):\n",
    "    \"\"\"Specialized class of Model for NER\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(NERModel, self).__init__(config)\n",
    "        self.idx_to_tag = {idx: tag for tag, idx in\n",
    "                           self.config.vocab_tags.items()}\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        \"\"\"Define placeholders = entries to computational graph\"\"\"\n",
    "        \n",
    "        # shape = (batch size, max length of sentence in batch)\n",
    "        self.word_ids = tf.placeholder(tf.int32, shape=[None, None],\n",
    "                        name=\"word_ids\")\n",
    "\n",
    "        # shape = (batch size)\n",
    "        self.word_sequence_lengths = tf.placeholder(tf.int32, shape=[None],\n",
    "                        name=\"word_sequence_lengths\")\n",
    "        \n",
    "        # Define char placeholders for char representation\n",
    "        self.char_ids = tf.placeholder(tf.int32, shape = [None, None, None], #batch_size, max(len(word)), max(len(sentence))\n",
    "                                       name = 'char_ids')\n",
    "        \n",
    "        self.char_sequence_lengths = tf.placeholder(tf.int32, shape = [None, None], #batch_size, word \n",
    "                                                    name = 'char_sequence_lengths')\n",
    "\n",
    "        # shape = (batch size, max length of sentence in batch)\n",
    "        self.labels = tf.placeholder(tf.int32, shape=[None, None],\n",
    "                        name=\"labels\")\n",
    "\n",
    "\n",
    "        # hyper parameters\n",
    "        self.dropout = tf.placeholder(dtype=tf.float32, shape=[],\n",
    "                        name=\"dropout\") #this is to prevent overfitting \n",
    "        self.lr = tf.placeholder(dtype=tf.float32, shape=[],\n",
    "                        name=\"lr\") #learning rate\n",
    "\n",
    "    def get_feed_dict(self, words, labels=None, lr=None, dropout=None):\n",
    "        \"\"\"Given some data, pad it and build a feed dictionary\n",
    "\n",
    "        Args:\n",
    "            words: list of sentences. A sentence is a list of ids of a list of\n",
    "                words. A word is a list of ids\n",
    "            labels: list of ids\n",
    "            chars\n",
    "            lr: (float) learning rate\n",
    "            dropout: (float) keep prob\n",
    "\n",
    "        Returns:\n",
    "            dict {placeholder: value}\n",
    "\n",
    "        \"\"\"\n",
    "        # perform padding of the given data\n",
    "        if self.config.use_chars:\n",
    "            char_ids ,char_sequence_lengths = pad_sequences(char_ids, 0, 2)\n",
    "        else:\n",
    "            word_ids, word_sequence_lengths = pad_sequences(words, 0)\n",
    "\n",
    "        # build feed dictionary\n",
    "        feed = {\n",
    "            self.word_ids: word_ids,\n",
    "            self.word_sequence_lengths: word_sequence_lengths\n",
    "        }\n",
    "\n",
    "        if labels is not None:\n",
    "            labels, _ = pad_sequences(labels, 0)\n",
    "            feed[self.labels] = labels\n",
    "\n",
    "        if lr is not None:\n",
    "            feed[self.lr] = lr\n",
    "\n",
    "        if dropout is not None:\n",
    "            feed[self.dropout] = dropout\n",
    "\n",
    "        return feed, word_sequence_lengths\n",
    "\n",
    "    def add_word_embeddings_op(self):\n",
    "        with tf.variable_scope(\"words\"):\n",
    "            if self.config.embeddings is None:\n",
    "                self.logger.info(\"WARNING: randomly initializing word vectors\")\n",
    "                _word_embeddings = tf.get_variable(\n",
    "                        name=\"_word_embeddings\",\n",
    "                        dtype=tf.float32,\n",
    "                        shape=[self.config.nwords, self.config.dim_word])\n",
    "            else:\n",
    "                _word_embeddings = tf.Variable(\n",
    "                        self.config.embeddings,\n",
    "                        name=\"_word_embeddings\",\n",
    "                        dtype=tf.float32,\n",
    "                        trainable=self.config.train_embeddings)\n",
    "\n",
    "            word_embeddings = tf.nn.embedding_lookup(_word_embeddings,\n",
    "                    self.word_ids, name=\"word_embeddings\")\n",
    "            \n",
    "            # shape = (batch_size, max length of sentence in batch, word dimension)\n",
    "            self.word_embeddings =  tf.nn.dropout(word_embeddings, self.dropout)\n",
    "        \n",
    "        # define char_embeddings\n",
    "            with tf.variable_scope('chars'):\n",
    "                if self.config.use_chars:\n",
    "                    _char_embeddings = tf.get_variable(\n",
    "                        name = '_char_embeddings',\n",
    "                        dtype = tf.float32,\n",
    "                        shape = [self.config.nchars, self.config.dim_char])\n",
    "                    char_embeddings = tf.nn.embedding_lookup(_char_embeddings,\n",
    "                        self.char_ids, name = 'char_embeddings')\n",
    "\n",
    "                    s = tf.shape(char_embeddings)\n",
    "                    char_embeddings = tf.reshape(char_embeddings, shape = [s[0]*s[1], s[-2], self.config.dim_char])\n",
    "                    char_sequence_lengths = tf.reshape(self.char_sequence_lengths, shape = s[0]*s[1])\n",
    "            \n",
    "                    cell_fw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_char)\n",
    "                    cell_bw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_char)\n",
    "                    (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "                            cell_fw, cell_bw, char_embeddings,\n",
    "                            sequence_length=char_sequence_lengths, dtype=tf.float32)\n",
    "         \n",
    "        \n",
    "        # concatenate word_embeddings with char_embeddings\n",
    "                    output = tf.concat([output_fw, output_bw], axis=-1)\n",
    "                    all_embeddings = tf.concat([word_embeddings, output], axis = -1)\n",
    "            self.all_embeddings = tf.nn.dropout(word_embeddings, self.dropout)\n",
    "\n",
    "    def add_logits_op(self):\n",
    "        \"\"\"Defines self.logits\n",
    "\n",
    "        For each word in each sentence of the batch, it corresponds to a vector\n",
    "        of scores, of dimension equal to the number of tags.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"bi-lstm\"):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_lstm)\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_lstm)\n",
    "            (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "                    cell_fw, cell_bw, self.all_embeddings,\n",
    "                    sequence_length=self.word_sequence_lengths, dtype=tf.float32)\n",
    "            output = tf.concat([output_fw, output_bw], axis=-1)\n",
    "            output = tf.nn.dropout(output, self.dropout)\n",
    "\n",
    "        with tf.variable_scope(\"proj\"):\n",
    "            W = tf.get_variable(\"W\", dtype=tf.float32,\n",
    "                    shape=[2*self.config.hidden_size_lstm, self.config.ntags])\n",
    "\n",
    "            b = tf.get_variable(\"b\", shape=[self.config.ntags],\n",
    "                    dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "\n",
    "            nsteps = tf.shape(output)[1]\n",
    "            output = tf.reshape(output, [-1, 2*self.config.hidden_size_lstm])\n",
    "            pred = tf.matmul(output, W) + b\n",
    "            self.logits = tf.reshape(pred, [-1, nsteps, self.config.ntags])\n",
    "            self.labels_pred = tf.cast(tf.argmax(self.logits, axis=-1),\n",
    "                                       tf.int32)\n",
    "\n",
    "    def add_pred_op(self):\n",
    "        \"\"\"Defines self.labels_pred\n",
    "\n",
    "        This op is defined only in the case where we don't use a CRF since in\n",
    "        that case we can make the prediction \"in the graph\" (thanks to tf\n",
    "        functions in other words). With theCRF, as the inference is coded\n",
    "        in python and not in pure tensroflow, we have to make the prediciton\n",
    "        outside the graph.\n",
    "        \"\"\"\n",
    "        if not self.config.use_crf:\n",
    "            self.labels_pred = tf.cast(tf.argmax(self.logits, axis=-1),\n",
    "                    tf.int32)\n",
    "\n",
    "\n",
    "    def add_loss_op(self):\n",
    "        \n",
    "        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=self.logits, labels=self.labels)\n",
    "        mask = tf.sequence_mask(self.word_sequence_lengths)\n",
    "        losses = tf.boolean_mask(losses, mask)\n",
    "        self.loss = tf.reduce_mean(losses)\n",
    "        # for tensorboard\n",
    "        tf.summary.scalar(\"loss\", self.loss)\n",
    "\n",
    "    def build(self):\n",
    "        # NER specific functions\n",
    "        self.add_placeholders()\n",
    "        self.add_word_embeddings_op()\n",
    "        self.add_logits_op()\n",
    "        self.add_pred_op()\n",
    "        self.add_loss_op()\n",
    "\n",
    "        # Generic functions that add training op and initialize session\n",
    "        self.add_train_op(self.config.lr_method, self.lr, self.loss,\n",
    "                self.config.clip)\n",
    "        self.initialize_session() # now self.sess is defined and vars are init\n",
    "\n",
    "\n",
    "    def predict_batch(self, words):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            words: list of sentences\n",
    "\n",
    "        Returns:\n",
    "            labels_pred: list of labels for each sentence\n",
    "            word_sequence_length\n",
    "\n",
    "        \"\"\"\n",
    "        fd, word_sequence_lengths = self.get_feed_dict(words, dropout=1.0)\n",
    "        \n",
    "        labels_pred = self.sess.run(self.labels_pred, feed_dict=fd)\n",
    "\n",
    "        return labels_pred, word_sequence_lengths\n",
    "\n",
    "\n",
    "    def run_epoch(self, train, dev, epoch):\n",
    "        \"\"\"Performs one complete pass over the train set and evaluate on dev\n",
    "\n",
    "        Args:\n",
    "            train: dataset that yields tuple of sentences, tags\n",
    "            dev: dataset\n",
    "            epoch: (int) index of the current epoch\n",
    "\n",
    "        Returns:\n",
    "            f1: (python float), score to select model on, higher is better\n",
    "\n",
    "        \"\"\"\n",
    "        # progbar stuff for logging\n",
    "        batch_size = self.config.batch_size\n",
    "        nbatches = (len(train) + batch_size - 1) // batch_size\n",
    "        prog = Progbar(target=nbatches)\n",
    "\n",
    "        # iterate over dataset\n",
    "        for i, (words, labels) in enumerate(minibatches(train, batch_size)):\n",
    "            fd, _ = self.get_feed_dict(words, labels, self.config.lr,\n",
    "                    self.config.dropout)\n",
    "\n",
    "            _, train_loss, summary = self.sess.run(\n",
    "                    [self.train_op, self.loss, self.merged], feed_dict=fd)\n",
    "\n",
    "            prog.update(i + 1, [(\"train loss\", train_loss)])\n",
    "\n",
    "            # tensorboard\n",
    "            if i % 10 == 0:\n",
    "                self.file_writer.add_summary(summary, epoch*nbatches + i)\n",
    "\n",
    "        metrics = self.run_evaluate(dev)\n",
    "        msg = \" - \".join([\"{} {:04.2f}\".format(k, v)\n",
    "                for k, v in metrics.items()])\n",
    "        self.logger.info(msg)\n",
    "\n",
    "        return metrics[\"f1\"]\n",
    "\n",
    "    def run_evaluate(self, test):\n",
    "        \"\"\"Evaluates performance on test set\n",
    "\n",
    "        Args:\n",
    "            test: dataset that yields tuple of (sentences, tags)\n",
    "\n",
    "        Returns:\n",
    "            metrics: (dict) metrics[\"acc\"] = 98.4, ...\n",
    "\n",
    "        \"\"\"\n",
    "        accs = []\n",
    "        correct_preds, total_correct, total_preds = 0., 0., 0.\n",
    "        for words, labels in minibatches(test, self.config.batch_size):\n",
    "            labels_pred, word_sequence_lengths = self.predict_batch(words)\n",
    "\n",
    "            for lab, lab_pred, length in zip(labels, labels_pred,\n",
    "                                             word_sequence_lengths):\n",
    "                lab      = lab[:length]\n",
    "                lab_pred = lab_pred[:length]\n",
    "                accs    += [a==b for (a, b) in zip(lab, lab_pred)]\n",
    "\n",
    "                lab_chunks      = set(get_chunks(lab, self.config.vocab_tags))\n",
    "                lab_pred_chunks = set(get_chunks(lab_pred,\n",
    "                                                 self.config.vocab_tags))\n",
    "\n",
    "                correct_preds += len(lab_chunks & lab_pred_chunks)\n",
    "                total_preds   += len(lab_pred_chunks)\n",
    "                total_correct += len(lab_chunks)\n",
    "\n",
    "        p   = correct_preds / total_preds if correct_preds > 0 else 0\n",
    "        r   = correct_preds / total_correct if correct_preds > 0 else 0\n",
    "        f1  = 2 * p * r / (p + r) if correct_preds > 0 else 0\n",
    "        acc = np.mean(accs)\n",
    "\n",
    "        return {\"acc\": 100*acc, \"f1\": 100*f1}\n",
    "\n",
    "    def predict(self, words_raw):\n",
    "        \"\"\"Returns list of tags\n",
    "\n",
    "        Args:\n",
    "            words_raw: list of words (string), just one sentence (no batch)\n",
    "\n",
    "        Returns:\n",
    "            preds: list of tags (string), one for each word in the sentence\n",
    "\n",
    "        \"\"\"\n",
    "        words = [self.config.processing_word(w) for w in words_raw]\n",
    "        if type(words[0]) == tuple:\n",
    "            words = zip(*words)\n",
    "        pred_ids, _ = self.predict_batch([words])\n",
    "        preds = [self.idx_to_tag[idx] for idx in list(pred_ids[0])]\n",
    "\n",
    "        return preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b30b0db3a0d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mtrain_main\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-b30b0db3a0d5>\u001b[0m in \u001b[0;36mtrain_main\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_main\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# create instance of config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# build model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Config' is not defined"
     ]
    }
   ],
   "source": [
    "# %load train.py\n",
    "\n",
    "def train_main():\n",
    "    # create instance of config\n",
    "    config = Config()\n",
    "\n",
    "    # build model\n",
    "    model = NERModel(config)\n",
    "    model.build()\n",
    "\n",
    "    # create datasets\n",
    "    dev   = CoNLLDataset(config.filename_dev, config.processing_word,\n",
    "                         config.processing_tag, config.max_iter)\n",
    "    train = CoNLLDataset(config.filename_train, config.processing_word,\n",
    "                         config.processing_tag, config.max_iter)\n",
    "\n",
    "    # train model\n",
    "    model.train(train, dev)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "Initializing tf session\n",
      "Reloading the latest trained model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from results/test/model.weights/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from results/test/model.weights/\n",
      "Testing model over test set\n",
      "acc 96.61 - f1 84.31\n",
      "\n",
      "This is an interactive mode.\n",
      "To exit, enter 'exit'.\n",
      "You can enter a sentence like\n",
      "input> I love Paris\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input> I love Paris\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I love Paris \n",
      "O O    B-LOC \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input> Teddy Roosevelt is a president\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Teddy Roosevelt is a president \n",
      "B-PER I-PER     O  O O         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input> Tedddy Roosevelt is a president\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tedddy Roosevelt is a president \n",
      "B-PER  B-PER     O  O O         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input> I live in Neeeew York\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I live in Neeeew York  \n",
      "O O    O  B-LOC  I-LOC \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input> I live in New York\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I live in New   York  \n",
      "O O    O  B-LOC I-LOC \n"
     ]
    }
   ],
   "source": [
    "# %load evaluate.py\n",
    "from model.data_utils import CoNLLDataset\n",
    "from model.ner_model2 import NERModel\n",
    "from model.config import Config\n",
    "\n",
    "\n",
    "\n",
    "def align_data(data):\n",
    "    \"\"\"Given dict with lists, creates aligned strings\n",
    "\n",
    "    Adapted from Assignment 3 of CS224N\n",
    "\n",
    "    Args:\n",
    "        data: (dict) data[\"x\"] = [\"I\", \"love\", \"you\"]\n",
    "              (dict) data[\"y\"] = [\"O\", \"O\", \"O\"]\n",
    "\n",
    "    Returns:\n",
    "        data_aligned: (dict) data_align[\"x\"] = \"I love you\"\n",
    "                           data_align[\"y\"] = \"O O    O  \"\n",
    "\n",
    "    \"\"\"\n",
    "    spacings = [max([len(seq[i]) for seq in data.values()])\n",
    "                for i in range(len(data[list(data.keys())[0]]))]\n",
    "    data_aligned = dict()\n",
    "\n",
    "    # for each entry, create aligned string\n",
    "    for key, seq in data.items():\n",
    "        str_aligned = \"\"\n",
    "        for token, spacing in zip(seq, spacings):\n",
    "            str_aligned += token + \" \" * (spacing - len(token) + 1)\n",
    "\n",
    "        data_aligned[key] = str_aligned\n",
    "\n",
    "    return data_aligned\n",
    "\n",
    "\n",
    "\n",
    "def interactive_shell(model):\n",
    "    \"\"\"Creates interactive shell to play with model\n",
    "\n",
    "    Args:\n",
    "        model: instance of NERModel\n",
    "\n",
    "    \"\"\"\n",
    "    model.logger.info(\"\"\"\n",
    "This is an interactive mode.\n",
    "To exit, enter 'exit'.\n",
    "You can enter a sentence like\n",
    "input> I love Paris\"\"\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # for python 2\n",
    "            sentence = raw_input(\"input> \")\n",
    "        except NameError:\n",
    "            # for python 3\n",
    "            sentence = input(\"input> \")\n",
    "\n",
    "        words_raw = sentence.strip().split(\" \")\n",
    "\n",
    "        if words_raw == [\"exit\"]:\n",
    "            break\n",
    "\n",
    "        preds = model.predict(words_raw)\n",
    "        to_print = align_data({\"input\": words_raw, \"output\": preds})\n",
    "\n",
    "        for key, seq in to_print.items():\n",
    "            model.logger.info(seq)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # create instance of config\n",
    "    config = Config()\n",
    "\n",
    "    # build model\n",
    "    model = NERModel(config)\n",
    "    model.build()\n",
    "    model.restore_session(config.dir_model)\n",
    "\n",
    "    # create dataset\n",
    "    test  = CoNLLDataset(config.filename_test, config.processing_word,\n",
    "                         config.processing_tag, config.max_iter)\n",
    "\n",
    "    # evaluate and interact\n",
    "    model.evaluate(test)\n",
    "    interactive_shell(model)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
